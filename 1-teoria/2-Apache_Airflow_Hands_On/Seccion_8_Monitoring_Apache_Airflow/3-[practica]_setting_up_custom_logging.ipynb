{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8.3 - [Practica] Setting up custom logging**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous video, we have discovered how the logging system in Airflow works.\n",
    "\n",
    "In this video I’m going to show you the logging system in action and how can you customise it.\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "First, check that you are under the folder airflow-materials/airflow-section-8 and\n",
    "\n",
    "start the docker containers running Airflow with the command docker-compose -f\n",
    "\n",
    "docker-compose-CeleryExecutor.yml up -d,\n",
    "\n",
    "Enter.\n",
    "\n",
    "Ok, check that the containers are running with the command docker ps.\n",
    "\n",
    "Perfect. Now we are going to see the logs produced by the scheduler. Type the command docker logs\n",
    "\n",
    "and copy and paste the container id of the scheduler.\n",
    "\n",
    "Enter. Here you obtain the logs of the scheduler.\n",
    "\n",
    "Nothing new, you should already familiar with them.\n",
    "\n",
    "Let’s move to the code editor. Open the file airflow.cfg in the folder airflow-materials/airflow\n",
    "\n",
    "-section-8/mnt/airflow.\n",
    "\n",
    "Ok. From there, let me point out some important parameters.\n",
    "\n",
    "First, the parameter base_log_folder specifies where the log files will be stored.\n",
    "\n",
    "Then if you scroll down, we have the loglevel which is set to INFO by default.\n",
    "\n",
    "If we change this value by WARNING\n",
    "\n",
    "and save the file. From your terminal, execute the script, restart.sh.\n",
    "\n",
    "Enter.\n",
    "\n",
    "Now Airflow is restarted, type docker ps\n",
    "\n",
    "then docker logs with the container id of the scheduler.\n",
    "\n",
    "And ss you can see now, we have much less logs since we only keep the warning messages.\n",
    "\n",
    "Okay back to your code editor,\n",
    "\n",
    "change WARNING by INFO as before.\n",
    "\n",
    "Next,\n",
    "\n",
    "fab_logging_level defines the logging level of the Flask-appbuilder UI. Since the\n",
    "\n",
    "webserver of Airflow is based on Flask and built using the Flask\n",
    "\n",
    "-appbuilder framework, you could get additional logs based on this level.\n",
    "\n",
    "Nonetheless, keep the value by default. Just below, there is the parameter logging_config\n",
    "\n",
    "_class. This parameter allows you to define the class describing your logging configuration.\n",
    "\n",
    "Don't worry I will come back at it in a minute.\n",
    "\n",
    "Finally we have the parameters related to the log format and the log filename format.\n",
    "\n",
    "These three lines will be applied only if your terminal is a TTY, to apply fancy colours. The parameter\n",
    "\n",
    "log_format with the following string defines the format of your log.\n",
    "\n",
    "So basically, the format given here with the date time, the filename, the log level and so on, gives the log\n",
    "\n",
    "line that you have seen from the logs. In your terminal,\n",
    "\n",
    "if you scroll up,\n",
    "\n",
    "at some point you will the following line corresponding to the format.\n",
    "\n",
    "Ok back to the code editor, let’s change the format.\n",
    "\n",
    "The keywords that can you see here, are actually logrecord attributes given by the python logging\n",
    "\n",
    "module.\n",
    "\n",
    "If you want the exhaustive list, please check the link below.\n",
    "\n",
    "Okay let's say we would like to know from which process id the log has been produced as well as the\n",
    "\n",
    "name of the logger used to log the call. To do this,\n",
    "\n",
    "just after asctime type following string [ %%(process)s - %%(name)s ].\n",
    "\n",
    "Save the file and go back to your terminal. Execute the script restart.sh in order to apply the\n",
    "\n",
    "modifications.\n",
    "\n",
    "All right,\n",
    "\n",
    "now Airflow is running again, type docker\n",
    "\n",
    "ps,\n",
    "\n",
    "and docker logs with the container id of the scheduler.\n",
    "\n",
    "As you can see here, we have the process id as well as the name of the logger\n",
    "\n",
    "airflow.settings. By the way,\n",
    "\n",
    "notice that all the logs are not impacted by the format we just defined. As shown by the logs above,\n",
    "\n",
    "those logs are system logs and so depends on another logger object.\n",
    "\n",
    "Alright, we have seen the very basics of how to customise the logging system of Airflow,\n",
    "\n",
    "now it’s time to move to the advanced part. When you want to really customize the logging system of airflow\n",
    "\n",
    "you have to create a python file describing the configuration you want.\n",
    "\n",
    "This python file is a dictionary-formatted file where the content can be found from the repository of\n",
    "\n",
    "Airflow. Open your web browser and type the following link https://github.com/apache/airflow/blob/v1-10-stable/airflow/config_templates/airflow_local_settings.py.\n",
    "\n",
    "This file looks pretty complicated but don’t worry I’m gonna explain everything you need step by step.\n",
    "\n",
    "airflow_local_settings.py contains the formatters, the type of handler\n",
    "\n",
    "to use according to the logger object printing out the log event, the level of log messages you want and\n",
    "\n",
    "so on.\n",
    "\n",
    "For example, the logger “airflow.task”, which is the one producing the logs related to the tasks, is\n",
    "\n",
    "defined with the handler “task” and the logging level INFO.\n",
    "\n",
    "Notice that LOG_LEVEL here refers to the LOG_LEVEL parameter in airflow.cfg.\n",
    "\n",
    "Now, if we check the handler “task” just above, we can see that the FileHandler is used with the formatter\n",
    "\n",
    "“airflow”. The logs will be stored at the location defined by BASE_LOG_FOLDER\n",
    "\n",
    "with the filename templated with FILENAME_TEMPLATE as defined in the airflow.cfg configuration\n",
    "\n",
    "file.\n",
    "\n",
    "Let me show you quickly those values.\n",
    "\n",
    "Here\n",
    "\n",
    "and here.\n",
    "\n",
    "All right,\n",
    "\n",
    "last thing before moving forward,\n",
    "\n",
    "if we take a look at the formatter “airflow”, we can see that the LOG_FORMAT parameter we set\n",
    "\n",
    "earlier is used as well. As a best practice you should use this file instead of the airflow.cfg file\n",
    "\n",
    "in order to customize the logging system of airflow.\n",
    "\n",
    "Ok now it’s time to create your own custom configuration. First, copy the content of this file\n",
    "\n",
    "and go to your code editor.\n",
    "\n",
    "From there, create a new file called log_config.py under the folder mnt/airflow\n",
    "\n",
    "/conf.\n",
    "\n",
    "Like that.\n",
    "\n",
    "Then paste the code you copied and save the file.\n",
    "\n",
    "All right several things to explain here.\n",
    "\n",
    "When you want to customize the logging system of Airflow, you have to create a folder called “conf” at\n",
    "\n",
    "the default location /usr/local/airflow.\n",
    "\n",
    "In this folder you have to put two files. “Log_config.py” corresponding to the custom\n",
    "\n",
    "configuration of the logging system and the file __init__\n",
    "\n",
    ".py to make the log_config.py importable.\n",
    "\n",
    "Notice that the folder conf here is bound to the docker containers running Airflow.\n",
    "\n",
    "Indeed, if you open the dockerfile docker-compose-CeleryExecutor.yml, the services running Airflow\n",
    "\n",
    "have the volume mnt/airflow/conf from the host binded with /usr/local/airflow\n",
    "\n",
    "/conf in their containers as shown here.\n",
    "\n",
    "Notice the PYTHONPATH set here, to tell python where to find the log_config module.\n",
    "\n",
    "Alright, now everything is set up,\n",
    "\n",
    "open the file airflow.cfg, and look for the parameter logging_config_class.\n",
    "\n",
    "Here, you have to specify the class describing the logging configuration. In our case it’s log_\n",
    "\n",
    "config.DEFAULT_LOGGING_CONFIG.\n",
    "\n",
    "Save the file.\n",
    "\n",
    "Notice that this class is the big dictionary that you can find from the file log_config.\n",
    "\n",
    "py we created.\n",
    "\n",
    "So back to the file look for DEFAULT_LOGGING_CONFIG,\n",
    "\n",
    "and here it is with the formatters, handlers and loggers defined.\n",
    "\n",
    "All right, let’s check if the custom configuration file is taken into account by Airflow.\n",
    "\n",
    "Go to your terminal and execute the script restart.sh.\n",
    "\n",
    "Now type docker\n",
    "\n",
    "ps\n",
    "\n",
    "and docker logs with the container id of the scheduler.\n",
    "\n",
    "If you see the following line telling you that the user-defined logging config has been successfully imported\n",
    "\n",
    "then well done!\n",
    "\n",
    "It means that everything works.\n",
    "\n",
    "Otherwise, rewatch the video to check if you didn’t make any mistakes.\n",
    "\n",
    "All right,\n",
    "\n",
    "I would like to show you one more thing before moving to the next video. From your web browser,\n",
    "\n",
    "open a new tab and go to localhost\n",
    "\n",
    ":8080.\n",
    "\n",
    "Then, turn on the toggle of the DAG logger_dag so that the tasks start executing.\n",
    "\n",
    "Now, in your code editor,\n",
    "\n",
    "open the folder “logs” from the left panel here. There are three folders in the logs.\n",
    "\n",
    "“Dag_processor_manager” contains the logs related to the processing by Airflow\n",
    "\n",
    "of your DAGs. Given a list of DAG definition files,\n",
    "\n",
    "the dag processor manager is in charge of parsing and analyzing your DAGs to see what tasks should\n",
    "\n",
    "run,\n",
    "\n",
    "creating appropriate tasks instances in the database, recording any errors, killing any task instances\n",
    "\n",
    "belonging to the DAGs that haven’t issued a heartbeat in a while and so on.\n",
    "\n",
    "If you open the file dag_processor_manager.log, you can see that the DAGs get\n",
    "\n",
    "processed at an interval of time defined by the parameter min_file_process\n",
    "\n",
    "_interval in airflow.cfg.\n",
    "\n",
    "Okay next the folder “scheduler” contains the logs related to the schedule of your tasks.\n",
    "\n",
    "If we look for the task t1 which is a task of the DAG logging_dag, you can see when the\n",
    "\n",
    "task has been scheduled and triggered.\n",
    "\n",
    "Finally, when you schedule a DAG, a folder with the same name of that DAG is created in the logs.\n",
    "\n",
    "Then a folder is created for each task with their corresponding logs.\n",
    "\n",
    "For example, if we click on logger_dag, t1, and the most recent execution date, then open\n",
    "\n",
    "the file 1.log,\n",
    "\n",
    "we obtain the output produced by the execution of the task t1. Notice that the 1 here corresponds to\n",
    "\n",
    "how many times the task has been tried.\n",
    "\n",
    "So here, t1 has been executed one time.\n",
    "\n",
    "If the task was retried 2 times, then we would have 2.log instead of 1.log.\n",
    "\n",
    "Ok now, go back to file log_config.py and scroll down until you reach DEFAULT\n",
    "\n",
    "_DAG_PARSING_LOGGING_CONFIG.\n",
    "\n",
    "This dictionary allows you to customise the logs of the dag processor manager. From the file dag\n",
    "\n",
    "_processor_manager.log, we have the logs with level INFO. Let’s\n",
    "\n",
    "modify this. In log_config.py change the level here from LOG_LEVEL\n",
    "\n",
    "to the string ‘CRITICAL’\n",
    "\n",
    "and save the file. In your terminal, stop the containers by executing stop.sh.\n",
    "\n",
    "Ok, now in your code editor, delete the file dag_processor_manager.log. Like that.\n",
    "\n",
    "And start the docker containers again with start.sh.\n",
    "\n",
    "All right,\n",
    "\n",
    "back to the code editor, the file dag_processor_manager.log has been recreated,\n",
    "\n",
    "but if you open it, this time you got nothing.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because thanks to the modification we made, we only keep the critical log messages and we don't have\n",
    "\n",
    "any yet.\n",
    "\n",
    "That’s an example of how you can customize each logger of Airflow and set their own log level which is\n",
    "\n",
    "not possible from the file airflow.cfg.\n",
    "\n",
    "Okay so let's revert the modifications we made.\n",
    "\n",
    "Save the file.\n",
    "\n",
    "And restart the containers by executing restart.sh.\n",
    "\n",
    "All right,\n",
    "\n",
    "that’s it for this long video, a lot of information was given here. In the next video we are going to discover\n",
    "\n",
    "how to write and read the Airflow logs with AWS S3.\n",
    "\n",
    "Take a quick break and see you for the next video.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
