{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8.5 - Elasticsearch Reminder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this video, I’m going to give you a quick reminder of what is Elasticsearch and how does it work.\n",
    "\n",
    "So, what is Elasticsearch? Elasticsearch is an open source search engine providing a distributed full\n",
    "\n",
    "-text search engine with an HTTP web interface and schema-free JSON documents.\n",
    "\n",
    "It allows you to store, search and analyze large volumes of data.\n",
    "\n",
    "It is well suited for collecting and aggregating log data at scale to look for trends,\n",
    "\n",
    "statistics, summarizations and more in near real time.\n",
    "\n",
    "How does it work?\n",
    "\n",
    "There are two concepts to know in Elasticsearch.\n",
    "\n",
    "The first one is the document.\n",
    "\n",
    "A document can be seen as a row in a relational database and it corresponds to your data stored in\n",
    "\n",
    "JSON format into Elasticsearch.\n",
    "\n",
    "So basically, a document could be the following JSON data coming from a log file with different fields\n",
    "\n",
    "such as Id, Name, and Loglevel as well as their associated values.\n",
    "\n",
    "Since your log file contains many log events, each log event is a document along with a unique id.\n",
    "\n",
    "Next,\n",
    "\n",
    "these documents are stored into an Index which is a collection of documents. You can think of an index\n",
    "\n",
    "as a database from a relational database perspective. For scalability reasons,\n",
    "\n",
    "it’s a best practice to create a new index for each day when you are dealing with log events in Elastic\n",
    "\n",
    "search.\n",
    "\n",
    "By doing this, you will be able to request your logs\n",
    "\n",
    "according to a given date or a range of dates, avoiding requesting all your data which would be very slow\n",
    "\n",
    "and non optimized.\n",
    "\n",
    "That’s why I set a date to the index name here. Then, an index can have one or more mapping types that\n",
    "\n",
    "are used to divide documents into logical groups inside the same index. A mapping defines how a document\n",
    "\n",
    "is indexed and how its fields those ones, are indexed and stored.\n",
    "\n",
    "You can think of a mapping as a table in a relational database.\n",
    "\n",
    "Basically, each document has a defined type as you can see from the field _type.\n",
    "\n",
    "Finally, once your documents are mapped and stored into an index, you can start requesting them through\n",
    "\n",
    "the REST API given by Elasticsearch.\n",
    "\n",
    "All right,\n",
    "\n",
    "one thing you have to know is that when we talk about Elasticsearch, we usually refer to the stack ELK\n",
    "\n",
    "for Elasticsearch, Logstash and Kibana. Indeed, Elasticsearch is the document-oriented database\n",
    "\n",
    "organizing your data but how can you actually ship your data into Elasticsearch and how once they are stored,\n",
    "\n",
    "how can visualize them?\n",
    "\n",
    "Well that’s where Logstash and Kibana\n",
    "\n",
    "now step in.\n",
    "\n",
    "Let’s begin with Kibana.\n",
    "\n",
    "Kibana is an open source data visualization tool used for log analytics, application monitoring and\n",
    "\n",
    "more.\n",
    "\n",
    "It gives you a powerful and easy way to make dashboards on top of your data stored in Elasticsearch.\n",
    "\n",
    "Don’t worry, we will see how to use Kibana in the next videos.\n",
    "\n",
    "The last component of the stack ELK is Logstash. Logstash is a server side data processing pipeline\n",
    "\n",
    "that can ingest data from multiple sources simultaneously, transform it and ship it to the output you\n",
    "\n",
    "want such as Elasticsearch, Statsd, Kafka and so on.\n",
    "\n",
    "In other words, it allows you to ingest data of different shapes and sources, then parse each event in\n",
    "\n",
    "order to build a common format to finally send the processed data to one or multiple systems.\n",
    "\n",
    "If you want to apply aggregations, filters or transformations to your log events before forwarding them,\n",
    "\n",
    "then Logstash could be useful for you.\n",
    "\n",
    "All right.\n",
    "\n",
    "Before moving to the practice, there is actually one more tool used in the stack ELK that I want to\n",
    "\n",
    "talk about which is Filebeat. Filebeat\n",
    "\n",
    "is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers,\n",
    "\n",
    "Filebeat monitors the log files or locations that you specify, collect log events, and forwards them to\n",
    "\n",
    "either Elasticsearch or Logstash for indexing. Like logstash,\n",
    "\n",
    "you have to define an input and an output but the transformations you can make on your data are very\n",
    "\n",
    "limited.\n",
    "\n",
    "Nonetheless, Filebeat has a very small CPU and memory footprint and that’s why it is usually installed\n",
    "\n",
    "on the same node where the logs are in order to forward them to logstash which is running on another node\n",
    "\n",
    "since it consumes a lot more resources.\n",
    "\n",
    "All right that's it for this video.\n",
    "\n",
    "No you have a better idea of what each tool does,\n",
    "\n",
    "let’s move to the practice we are going to set up everything in order to monitor Airflow with\n",
    "\n",
    "Elasticsearch.\n",
    "\n",
    "See you in the next video.\n",
    "8.6. [Practica] Configuring Airflow with Elasticsearch\n",
    "\n",
    "After having a quick reminder about Elasticsearch, Kibana, Logstash and Filebeat, in this video we are\n",
    "\n",
    "going to configure Airflow to work with Elasticsearch.\n",
    "\n",
    "Here is the architecture we are going to set up for writing and reading log\n",
    "\n",
    "events of Airflow in Elasticsearch.\n",
    "\n",
    "Basically, we will have 3 docker containers corresponding to Logstash, Elasticsearch, and Kibana\n",
    "\n",
    "as well as another container where the Airflow worker is running. Inside this container\n",
    "\n",
    "we will install Filebeat in order to fetch the logs and ship them to Logstash.\n",
    "\n",
    "One important point to keep in mind is that Airflow will not write the log events directly into Elastic\n",
    "\n",
    "search.\n",
    "\n",
    "I repeat, unlike with AWS S3 where Airflow was able to write the logs into S3, with Elastic\n",
    "\n",
    "search it can only read them and so you have to set up Filebeat in order to make the import.\n",
    "\n",
    "At the end, when a DAG will be triggered, the log events of a given task will be stored in JSON into\n",
    "\n",
    "local look files.\n",
    "\n",
    "These log files will be at the path given by the parameter base_log_foder\n",
    "\n",
    "which is /usr/local/airflow/logs by default.\n",
    "\n",
    "Then, each time a new log file is produced, Filebeat will process it,\n",
    "\n",
    "add an offset to each log event and send the output to Logstash. Logstash will get the logs\n",
    "\n",
    "and will apply some transformations in order to generate a log id field required by Airflow to finally\n",
    "\n",
    "ship them into Elasticsearch.\n",
    "\n",
    "Once the data are stored into Elasticsearch, we will be able to monitor our DAGs through Kibana by making\n",
    "\n",
    "beautiful Dashboards. Before moving to the practice\n",
    "\n",
    "there are two important points that you must care about. In order to read the logs from Elasticsearch,\n",
    "\n",
    "Airflow assumes two things. First,\n",
    "\n",
    "your log event should have a field called offset which will be used to display the logs in the right order.\n",
    "\n",
    "This offset is automatically created by Filebeat when a given file is processed.\n",
    "\n",
    "The second thing is that Airflow assumes that a field log_id corresponding to the concatenation\n",
    "\n",
    "of the dag_id, task_id, execution_date and try_number\n",
    "\n",
    "as shown here, is defined for each log events. The log_id field will be used by Airflow in order to\n",
    "\n",
    "retrieve the log from Elasticsearch.\n",
    "\n",
    "This field is not automatically created by Logstash and so you have to define some transformations\n",
    "\n",
    "that I already made for you in order to generate it.\n",
    "\n",
    "One last point. In order to use Elasticsearch with Airflow\n",
    "\n",
    "you have to install the package elasticsearch of Airflow like we do with S3. All right without further waiting,\n",
    "\n",
    "let’s move to the practice. From your code editor, check that you are under the folder airflow-materials\n",
    "\n",
    "/airflow-section-8 and open the file docker-compose-CeleryExecutor\n",
    "\n",
    "ELK.yml. Nothing new about Airflow, but if you scroll down,\n",
    "\n",
    "you can see the service Elasticsearch and Kibana. The environment variable SERVER_HOST\n",
    "\n",
    "must be set to 0.0.0.0 otherwise you won’t be able to access the UI of Kibana from\n",
    "\n",
    "your web browser. Just below, we have the service Losgstash, with the following volume. This volume corresponds\n",
    "\n",
    "to the folder containing the logstash pipelines where the logs will be shipped and processed. If you\n",
    "\n",
    "open the file airflow-logs.conf in the folder mnt/logstash/pipeline,\n",
    "\n",
    "here you have the pipeline that I made for you where the logs of Airflow will go through. This pipeline\n",
    "\n",
    "is divided in three parts. The input where we specify that the log event will come from Filebeat\n",
    "\n",
    "on the port 5044. Then, the filter where we define multiple transformations in order to parse\n",
    "\n",
    "the JSON data, generates the log_id field as shown here, and move the field offset to the\n",
    "\n",
    "root of the JSON document instead of being under the field log by default.\n",
    "\n",
    "Finally, the last part corresponding to the output, is where the processed log event is redirected to\n",
    "\n",
    "Elasticsearch. Notice the index format here so that each index name will begin with airflow-logs\n",
    "\n",
    "and the current date. Dividing your logs by day into Elasticsearch is considered as a best practice in order\n",
    "\n",
    "to optimize your requests.\n",
    "\n",
    "All right, now let’s configure Airflow to work with Elasicsearch. Open the file airflow.cfg. First, since\n",
    "\n",
    "we are going to use a remote storage for the logs, we have to set the parameter remote_logging\n",
    "\n",
    "to true.\n",
    "\n",
    "Now, unlike with the other remote storages such as S3 or GCP, we are not going to create a connection\n",
    "\n",
    "neither set a remote_base_log_folder. Indeed, look for the section “elastic\n",
    "\n",
    "search”.\n",
    "\n",
    "and here we have the different parameters that we need to define.\n",
    "\n",
    "Let us begin with the host which is http://elasticsearch:9200.\n",
    "\n",
    "Since we are using Docker, notice that elasticsearch here, corresponds to the service name running\n",
    "\n",
    "Elasticsearch which is elasticsearch. The next parameter to define is “write_stdout”.\n",
    "\n",
    "This parameter allows to write the task logs to the standard output of the worker instead of the default\n",
    "\n",
    "files. Since we actually want to store the logs in the default files, remove the value and let the parameter\n",
    "\n",
    "empty.\n",
    "\n",
    "This is important. Indeed,\n",
    "\n",
    "I don’t know if it is a bug, but if you keep the parameter sets to False, with Elasticsearch enabled,\n",
    "\n",
    "the log files won’t be produced.\n",
    "\n",
    "So for now, keep the parameter empty.\n",
    "\n",
    "Next, set the parameter json_format\n",
    "\n",
    "to True since Elasticsearch expects JSON data.\n",
    "\n",
    "Okay.\n",
    "\n",
    "The parameter json_fields here, shows the fields that will be added in addition to your\n",
    "\n",
    "log event.\n",
    "\n",
    "So we will have the timestamp of the log, the filename where it has been printed out and so on.\n",
    "\n",
    "You can find the list of all possible fields at the link below.\n",
    "\n",
    "Last but not least, the parameter log_id_template here corresponds to the\n",
    "\n",
    "format of the log id value used to retrieve the logs of a given task in Elasticsearch from the Airflow\n",
    "\n",
    "UI.\n",
    "\n",
    "By default, It is composed of the dag id, task id, execution date and number of tries, all separated by\n",
    "\n",
    "dashes.\n",
    "\n",
    "Keep in mind the template given here, should be the same as defined in the logstash pipeline.\n",
    "\n",
    "Okay\n",
    "\n",
    "So we are done with the config of Airflow,\n",
    "\n",
    "we can start the architecture. From your terminal, start the docker containers by typing “docker-compose -\n",
    "\n",
    "f\n",
    "\n",
    "docker-compose-CeleryExecutorELK.yml up -d”.\n",
    "\n",
    "Ok, type docker ps.\n",
    "\n",
    "As you can see, we have three new docker containers which are elasticsearch, kibana and logstash. Now\n",
    "\n",
    "let’s check that everything works fine. [Browser] From your web browser type localhost:9200.\n",
    "\n",
    "Enter.\n",
    "\n",
    "perfect, we obtain some information about Elasticsearch. Open a new tab, and type\n",
    "\n",
    "localhost:5601,\n",
    "\n",
    "Enter.\n",
    "\n",
    "And we got Kibana\n",
    "\n",
    "as expected. Last, in a new tab again, type localhost:8080,\n",
    "\n",
    "Enter. And the Airflow UI is running. Perfect so everything works\n",
    "\n",
    "let’s move forward. Back to your terminal. Connect to the airflow worker by typing “docker exec -it”\n",
    "\n",
    "the container id of the worker,\n",
    "\n",
    "“/bin/bash”.\n",
    "\n",
    "Enter. Okay\n",
    "\n",
    "at this point, Airflow is configured to read task logs from Elasticsearch.\n",
    "\n",
    "The last step is to set up Filebeat in order to ship the logs into Logstash to finally store them into\n",
    "\n",
    "Elasticsearch so that we will be able to read them from the Airflow UI.\n",
    "\n",
    "First, we need to download Filebeat with the command “curl -L -O\n",
    "\n",
    "and the following link https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.5.2-linux-x86_64.tar.gz”.\n",
    "\n",
    "Enter.\n",
    "\n",
    "Let’s wait for the download to finish.\n",
    "\n",
    "Okay.\n",
    "\n",
    "Extract the files with “tar xvzf”\n",
    "\n",
    "and the name of the file. Like that.\n",
    "\n",
    "Enter.\n",
    "\n",
    "Then go inside the new folder and type “ls”.\n",
    "\n",
    "All right, here we have the filebeat binary and the configuration file of Filebeat called filebeat\n",
    "\n",
    ".yml. Open it with “vim filebeat.yml”.\n",
    "\n",
    "The first thing we have to configure is where Filebeat should take a look to find the log files and\n",
    "\n",
    "how they should be processed.\n",
    "\n",
    "To do this, you have to specify an input. Filebeats brings many different inputs such as log files, Kafka,\n",
    "\n",
    "S3, Redis and so on.\n",
    "\n",
    "You can find the exhaustive list at the link below.\n",
    "\n",
    "In our case we keep the input log which is the one to use when we need to read lines from log files.\n",
    "\n",
    "Then, just below, we enable the input by setting the parameter to True.\n",
    "\n",
    "Next, we have to set the path\n",
    "\n",
    "where the log files are. If you remember when a task is executed, multiple folders are automatically created\n",
    "\n",
    "respectively corresponding to the dag id, the task id, the execution date and finally the log file\n",
    "\n",
    "with its name defined by the number of retries of that task.\n",
    "\n",
    "So here we replace this path\n",
    "\n",
    "by /usr/local/airflow/logs which is the default path of the log files.\n",
    "\n",
    "and we type “/*/*/*/*.log”.\n",
    "\n",
    "Here,\n",
    "\n",
    "each wildcard, correspond to the dag id, the task id, the execution date, and the file respectively.\n",
    "\n",
    "Check that the path is correct\n",
    "\n",
    "otherwise Filebeat will throw you a lot of errors telling you that the JSON format is incorrect, or won’t\n",
    "\n",
    "tell you anything if the path points somewhere where no log files exist.\n",
    "\n",
    "Last but not least, we have to set the output. Look for logstash by typing /logstash.\n",
    "\n",
    "Enter. Here, uncomment the line output.logstash\n",
    "\n",
    "as well as the line defining the hosts.\n",
    "\n",
    "Change localhost by logstash which is the service name of Logstash in the docker compose file.\n",
    "\n",
    "Like that. Then, just above, comment the lines hosts and output.Elasticsearch from the Elasticsearch\n",
    "\n",
    "output.\n",
    "\n",
    "Perfect. Save the file. Now everything is set up, we can start Filebeat. Exit the file by typing :Q\n",
    "\n",
    "and type ./filebeat -e -c\n",
    "\n",
    "filebeat.yml -d\n",
    "\n",
    "“publish”.\n",
    "\n",
    "double quotes. This command willl start Filebeat\n",
    "\n",
    "based on the configuration file filebeat.yml and enables the debug selector publish to filter\n",
    "\n",
    "the logs of Filebeat. Enter.\n",
    "\n",
    "Ok Filebeat is running. Let’s go the Airflow UI. Turn on the toggle of the DAG logger_dag\n",
    "\n",
    "and wait for the DAGRun to finish.\n",
    "\n",
    "Perfect.\n",
    "\n",
    "Now go Kibana and click on “Explore on my own”. Then, from the left panel, click on the last icon here\n",
    "\n",
    "and “Index Management”. As you can see, we have an index called airflow-logs with the current date corresponding\n",
    "\n",
    "to when the logs have been processed.\n",
    "\n",
    "Now click on “Index Patterns” just below Kibana and “Create Index Pattern”. From there,\n",
    "\n",
    "type the index pattern that should match with the Elasticsearch index below.\n",
    "\n",
    "So we type airflow-logs-*.\n",
    "\n",
    "Ok,\n",
    "\n",
    "the pattern matches the index. “Next step”.\n",
    "\n",
    "Here, we select @timestamp which is the field that will be used to filter the data by time.\n",
    "\n",
    "Notice that this field is automatically generated by Elasticsearch\n",
    "\n",
    "when a log event is stored. Click on “Create index pattern”.\n",
    "\n",
    "Perfect.\n",
    "\n",
    "Here we obtain the mapping of the log documents.\n",
    "\n",
    "Meaning, we are able to request these fields from Kibana to make filters, aggregations, statistics\n",
    "\n",
    "and more on the data. From the left panel,\n",
    "\n",
    "click on “Discover” just below the clock. And here they are. The log events of your tasks generated from\n",
    "\n",
    "Airflow are stored and queryable from Elasticsearch.\n",
    "\n",
    "Expand the first log by clicking here. If you scroll down, you can see the DAG id of the task,\n",
    "\n",
    "some info about the host where Filebeat is running, the log level, the log id, the offset, the task id\n",
    "\n",
    "and so on.\n",
    "\n",
    "Notice that you can see the transformation we made from Logstash to the log on the field offset.\n",
    "\n",
    "Here we have log.offset which is created by default by Filebeat and by applying our transformation\n",
    "\n",
    "we could see the same value but with the name offset.\n",
    "\n",
    "This is required since Airflow requests on a field called offset and not log.offset.\n",
    "\n",
    "Finally. If you go back to the Airflow, then click on logger_\n",
    "\n",
    "dag. “Graph View”.\n",
    "\n",
    "Then “t2” and “view log”. As you can see, Airflow is able to retrieve the logs from Elasticsearch. Perfect.\n",
    "\n",
    "So now you know how to set up everything to use Elasticsearch with Airflow.\n",
    "\n",
    "Now it’s time to build a Dashboard in order to monitor your DAGs using Kibana and Elasticsearch. So keep\n",
    "\n",
    "everything running and see you in the next video.\n",
    "\n",
    "8.7. [Practica] Monitoring your DAGs with Elasticsearch\n",
    "\n",
    "We have learned how to set up\n",
    "\n",
    "Airflow in order to write and read log events using Elasticsearch, now it’s time to monitor our DAGs\n",
    "\n",
    "with Kibana.\n",
    "\n",
    "The first thing we are going to do is to make some cleaning. From the Airflow UI, turn off the toggle\n",
    "\n",
    "of the DAG logger_dag and remove the tasks as well as the DAGRuns.\n",
    "\n",
    "Like that.\n",
    "\n",
    "Then, open your code editor, and check that you are under the folder airflow-materials/airflow\n",
    "\n",
    "-section-8. From there, open the folder mnt/airflow/logs and delete the folder\n",
    "\n",
    "logger_dag.\n",
    "\n",
    "Perfect. Now, go to Kibana and click on the last icon from the left panel. Then “Index Management”,\n",
    "\n",
    "select the index, click on “Manage Index”, “Delete Index” and confirm. Next, click on “Index Pattern”, select the pattern\n",
    "\n",
    "and click on the red button here to delete it.\n",
    "\n",
    "“delete”. All right.\n",
    "\n",
    "Everything is clean,\n",
    "\n",
    "we can move forward. In order to create a meaningful dashboard,\n",
    "\n",
    "we are going to generate some log events. Back to the Airflow UI, turn on the toggle of the dag data\n",
    "\n",
    "_dag. This DAG is composed of three tasks which basically do nothing except the last one that\n",
    "\n",
    "will fail\n",
    "\n",
    "half the time based on the current execution date.\n",
    "\n",
    "We can take a look at it by clicking on the DAG, then “Graph View”, “fail” and “Rendered”. This is the bash command\n",
    "\n",
    "executed by the task.\n",
    "\n",
    "Valid here corresponds to the day of the execution date. Since the start date of the DAG is set 10 days\n",
    "\n",
    "ago and the catchup parameter is enabled, the execution will change during the backfill process.\n",
    "\n",
    "So depending if the day is pair or not, either exit 1 or exit 0 will be executed.\n",
    "\n",
    "All right,\n",
    "\n",
    "click on Tree View. And the DAGRuns are finished.\n",
    "\n",
    "Okay it's done. From Kibana, click on “Index Management” and check that you have an index airflow-logs\n",
    "\n",
    "as shown here.\n",
    "\n",
    "Notice that the date would be different for you.\n",
    "\n",
    "So, the log events have been stored in Elasticsearch,\n",
    "\n",
    "Let's create the mapping again.\n",
    "\n",
    "Click on “Index Patterns”, “Create index pattern”. Type “airflow-logs-*”.\n",
    "\n",
    "The pattern and the index match. Click on “Next step”.\n",
    "\n",
    "Select the field “@timestamp” to be able to filter the logs by time and validate by clicking on “Create\n",
    "\n",
    "index pattern”. Perfect. Time to create the dashboard. A dashboard in Kibana is composed of multiple visualizations\n",
    "\n",
    "which are based on Elasticsearch requests.\n",
    "\n",
    "Let’s create the first one. From the left panel,\n",
    "\n",
    "click on “Visualize” and “Create new visualization”.\n",
    "\n",
    "Here, you can choose between multiple visualizations such as Gauge, Goal, Map, Heat Map and so on.\n",
    "\n",
    "In our case we choose the Gauge which indicates if a given value goes beyond a defined threshold.\n",
    "\n",
    "Basically, we are going to use it in order to monitor if a given DAG goes beyond a number of errors and so be warned\n",
    "\n",
    "because it may be in trouble. Here, we choose the source airflow-logs\n",
    "\n",
    "-*. Ok,\n",
    "\n",
    "Now\n",
    "\n",
    "on the left you have two options to customize your visualization. The Metrics which is Count by default but\n",
    "\n",
    "you can set an average, a sum and so on.\n",
    "\n",
    "Just below you can specify your label to the metric.\n",
    "\n",
    "Let us type “Number of failed tasks”.\n",
    "\n",
    "Then The Buckets here allows to split your visualization based on an aggregation. Click on “Add” and “Split\n",
    "\n",
    "group”.\n",
    "\n",
    "Select the aggregation Terms. And select the field dag_\n",
    "\n",
    "id.keyword. By doing this we are going to obtain one Gauge for each DAG id existing in the log events.\n",
    "\n",
    "To make things more clear,\n",
    "\n",
    "open the Airflow UI, and turn on the toggle of the DAG logger_dag. Refresh the page until the DAGRun\n",
    "\n",
    "is finished.\n",
    "\n",
    "Ok, back to Kibana, if you click on the button “Refresh” you can see\n",
    "\n",
    "that we have a new Gauge with the dag id logger_\n",
    "\n",
    "dag as shown here. The number here, correspond to the total number of log events\n",
    "\n",
    "since we don’t filter anything. Actually let’s apply a filter so that we only keep the tasks having failed.\n",
    "\n",
    "From the search bar, click on it, type “message” and select message.\n",
    "\n",
    "The difference between message and message.keyword is that first one allows you to make partial\n",
    "\n",
    "matching with a given string whereas .keyword will match only if the string is exactly the same.\n",
    "\n",
    "Now select the colon and type\n",
    "\n",
    "“Task\n",
    "\n",
    "exited with return code 1”.\n",
    "\n",
    "Finally, since our DAGs are scheduled to run every day,\n",
    "\n",
    "it would be better to have information on the past 7 days. To do this,\n",
    "\n",
    "click on the calendar here, and “Last 7 days”. Perfect. Since logger_dag doesn’t have any failed\n",
    "\n",
    "tasks,\n",
    "\n",
    "if you change 1 by 0 here so that you filter on the tasks having succeeded. Enter. We obtain the\n",
    "\n",
    "two gauges as expected.\n",
    "\n",
    "Now under your modification and change 0 by 1 again. Enter. And we obtain the number of tasks having failed\n",
    "\n",
    "for the dag\n",
    "\n",
    "data_dag. Since the number is below 50, it is still considered as working fine.\n",
    "\n",
    "Let’s say you want to define that above 10 errors\n",
    "\n",
    "the DAG should be considered in trouble.\n",
    "\n",
    "Well, if you click on “Options”, here you can define the different ranges. Replace the value 50 here by 10,\n",
    "\n",
    "meaning if the number of errors is greater or equal to 0 and less than 10,\n",
    "\n",
    "then the gauge will be in green.\n",
    "\n",
    "Delete the warning range.\n",
    "\n",
    "Then here, set 10 instead of 75 so that if the number of errors is greater than 10, then the gauge will\n",
    "\n",
    "be in red.\n",
    "\n",
    "Click on this button to apply the modifications. And the gauge is in red as expected.\n",
    "\n",
    "All right,\n",
    "\n",
    "now the visualization is set,\n",
    "\n",
    "click on “Save”\n",
    "\n",
    "here.\n",
    "\n",
    "Let's name it,\n",
    "\n",
    "“dag_id_gauge”\n",
    "\n",
    "and click on “Save” again.\n",
    "\n",
    "So we have created our first visualization, let's create a new one.\n",
    "\n",
    "Click on “Visualizations”, “Create visualization”. Select the vertical bar at the bottom.\n",
    "\n",
    "We select the index. Here, as we did with the Gauge,\n",
    "\n",
    "we create a new bucket on the x-axis.\n",
    "\n",
    "We select the aggregation “Terms” and the field “task_id.keyword”.\n",
    "\n",
    "Click on “Apply changes”. And we obtain the number of log events by tasks. Save the visualization,\n",
    "\n",
    "name it “task_id_vertical_bar”.\n",
    "\n",
    "“Save” again. Now go to “Dashboard”,\n",
    "\n",
    "“Create new dashboard”. Click on “Add”, then “dag_id_gauge” and “task_id_\n",
    "\n",
    "vertical_bar”. Close the panel. And as you can see, the visualizations have been added to the\n",
    "\n",
    "dashboard. Save it by clicking here, let’s give the name “Dag Monitoring”.\n",
    "\n",
    "Turn on “store time with dashboard” and click on “Save”. Perfect you just have created your first Dashboard\n",
    "\n",
    "and you are now able to monitor if a given DAG has too many failed tasks in the last 7 days as well\n",
    "\n",
    "as the number logs produced per task ids. As you may guess, Elasticsearch is extremely powerful and\n",
    "\n",
    "you can really move to the next level for monitoring your applications. I strongly recommend\n",
    "\n",
    "you to take a look at it. If you want more about monitoring Airflow with Elasticsearch, please\n",
    "\n",
    "let me know in the Q/A section, I will be glad to help you. All right, before moving forward, open your\n",
    "\n",
    "terminal, and check that you are under the folder airflow-materials/airflow-section-8.\n",
    "\n",
    "Then type the command “docker-compose -f\n",
    "\n",
    "docker-compose-CeleryExecutorELK.yml down”\n",
    "\n",
    "So that's it. I hope you enjoy what you have learned.\n",
    "\n",
    "Take a quick break and see you in the next video.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
