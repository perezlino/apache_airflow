{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8.6 - Introduction to metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we have seen how to monitor our DAGs based on log events using Filebeat, Logstash, Elastic\n",
    "\n",
    "search and Kibana.\n",
    "\n",
    "In this video we are going to discover how can we leverage metrics sent by Airflow in order to monitor\n",
    "\n",
    "it.\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "The first thing you need to know is that Airflow updates different metrics such as the scheduler heartbeats\n",
    "\n",
    "to know if it is still up, the time taken to scan and import all DAG files at once,\n",
    "\n",
    "the number of running tasks on executor and so on.\n",
    "\n",
    "All of these metrics can be sent to another tool called StatsD\n",
    "\n",
    "StatsD in order to expose them.\n",
    "\n",
    "So what is StatsD?\n",
    "\n",
    "StatsD is a simple deamon developed and released by Etsy in order to aggregate and summarize application\n",
    "\n",
    "metrics. You can think of StatsD as push-based monitoring system where it receives logs from an\n",
    "\n",
    "application and push them to somewhere else such as Elasticsearch or InfluxDB for example. Whenever\n",
    "\n",
    "Airflow wants to send a metric,\n",
    "\n",
    "this metric is sent over UDP datagrams to the StatsD daemon. Why UDP?\n",
    "\n",
    "Since this protocol doesn’t wait for acknowledgement like with TCP, it is extremely fast and there is\n",
    "\n",
    "no risk of blocking the application waiting for StatsD to receive the metric.\n",
    "\n",
    "The StatsD daemon keeps listening\n",
    "\n",
    "the UDP traffic, aggregates the metrics received and flushes them to the backend we specify at a defined\n",
    "\n",
    "interval.\n",
    "\n",
    "Again, the backend could be Elasticsearch or InfluxDB for example.\n",
    "\n",
    "Finally, StatsD is really simple to set up and to use. It has a tiny footprint in terms of resources\n",
    "\n",
    "and allows for decoupling your application from it,\n",
    "\n",
    "so if one crashes, the other is not impacted.\n",
    "\n",
    "Alright we have seen where Airflow sends its metrics, but we didn’t see what kind of metrics are sent\n",
    "\n",
    "actually. All metrics send by Airflow are based on three types which are Counters, Gauges and Timers. The\n",
    "\n",
    "type used depends on the metric expressed such as the number of overall task failures which is a Counter,\n",
    "\n",
    "the number of queued tasks on executor which is a Gauge, or the milliseconds taken to finish a task which\n",
    "\n",
    "is a timer.\n",
    "\n",
    "If you want the exhaustive list of metrics available for monitoring Airflow please take a look at the\n",
    "\n",
    "following link.\n",
    "\n",
    "So, now we know what metrics are produced and how they are sent,\n",
    "\n",
    "let’s take a look at what we are going to set up in the next video.\n",
    "\n",
    "Here is\n",
    "\n",
    "the schema of the architecture we are going to use in order to monitor Airflow.\n",
    "\n",
    "Basically, we are going to leverage the TIG stack. T stands for Telegraf, I for InfluxDB and G\n",
    "\n",
    "for Grafana.\n",
    "\n",
    "Let me quickly define each of these components.\n",
    "\n",
    "Telegraf, is an open source agent for collecting, processing and aggregating metrics.\n",
    "\n",
    "Once the data are collected from various kinds of inputs, they are pushed to different outputs such as\n",
    "\n",
    "InfluxDB, Elasticsearch, Syslog and so on.\n",
    "\n",
    "It is very lightweight and works with a system of plugins that you can add or remove from each step\n",
    "\n",
    "your metrics go through.\n",
    "\n",
    "There are the inputs, processors, aggregators and outputs.\n",
    "\n",
    "In our case, we are going to use the StatsD input plugin which runs the StatsD daemon to be able\n",
    "\n",
    "to receive metrics from Airflow.\n",
    "\n",
    "Then, we will send the metrics to InfluxDB. InfluxDB is an open source time series database built from\n",
    "\n",
    "the ground up to handle high write and query loads.\n",
    "\n",
    "The purpose of InfluxDB is to be used as a backing store for any use case involving large amounts\n",
    "\n",
    "of time stamped data.\n",
    "\n",
    "Since we can receive many metrics and they are based on time, InfluxDB will be perfect in our case.\n",
    "\n",
    "Finally, once the metrics are stored into InfluxDB, we will visualize and monitor them through\n",
    "\n",
    "Grafana. Like Kibana, Grafana is an open source data visualization and monitoring application.\n",
    "\n",
    "It supports many databases and gives the ability to create complex and beautiful dashboards mixing different\n",
    "\n",
    "data sources at one place. Grafana is also widely used to set alerts according to the metrics and\n",
    "\n",
    "thresholds\n",
    "\n",
    "you have. To sum up, the TIG stack is a really common stack for monitoring systems since each component\n",
    "\n",
    "works mostly with each other.\n",
    "\n",
    "All right, before moving to the practice, there are two important points that I have to tell you.\n",
    "\n",
    "First, the package statsd must be installed along with the other packages such as crypto, postgres\n",
    "\n",
    "and so on.\n",
    "\n",
    "Otherwise Airflow won’t be able to send the metrics.\n",
    "\n",
    "Finally, the parameter statsd_allow_list allows you to filter which metrics\n",
    "\n",
    "you want to send\n",
    "\n",
    "by specifying a list of prefixes corresponding to the metrics of Airflow.\n",
    "\n",
    "All right.\n",
    "\n",
    "That's it for this video and see you for the practice.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
