{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8.4 - [Practica] Storing your logs in AWS S3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learned how to set up a custom logging configuration in Airflow, now it’s time to see how to store\n",
    "\n",
    "and read logs on AWS S3. Before moving forward,\n",
    "\n",
    "let me remind you what is an AWS S3 bucket. A S3 bucket is a public cloud storage resource.\n",
    "\n",
    "A bucket is used to store objects, which consist of data and metadata that describes the data. You\n",
    "\n",
    "can think of it as a hard-drive on the cloud.\n",
    "\n",
    "That being said let's move forward. At this point,\n",
    "\n",
    "I assume that you already have an AWS account,\n",
    "\n",
    "So if not, please take your time to create one and come back when you are done. Ok,\n",
    "\n",
    "By default, Airflow stores its log files locally without compression.\n",
    "\n",
    "If you are running a lot of jobs or even a small number of jobs frequently, disk space can get eaten\n",
    "\n",
    "up pretty fast.\n",
    "\n",
    "Storing logs on S3 not only alleviates you from disk maintenance considerations, the durability of these\n",
    "\n",
    "files will be better guaranteed with AWS S3 versus what most disk manufactures and file systems\n",
    "\n",
    "can offer.\n",
    "\n",
    "All right, time to go to the AWS console.\n",
    "\n",
    "Once you are connected to your account, the first step is to create the S3 bucket where the logs of Airflow\n",
    "\n",
    "will be stored.\n",
    "\n",
    "Click on “Services”, type “S3” and select the first choice.\n",
    "\n",
    "Now, click on “Create bucket”.\n",
    "\n",
    "From there, we have to configure the bucket. The name here should be unique. In my case I’m gonna type marcl\n",
    "\n",
    "-airflow-logs.\n",
    "\n",
    "Notice that you have to set your own name. Don’t use the same as mine,\n",
    "\n",
    "It won't work.\n",
    "\n",
    "Okay.\n",
    "\n",
    "Select the closest region to your bucket. For me it’s Paris. Click on “Next”.\n",
    "\n",
    "Here we don't have to change anything.\n",
    "\n",
    "Next.\n",
    "\n",
    "Since we don’t want to allow public access to the bucket, we keep the box checked\n",
    "\n",
    "here. Ok\n",
    "\n",
    "Everything is good.\n",
    "\n",
    "We can click on “Create Bucket”, and the new bucket has been created here.\n",
    "\n",
    "Perfect.\n",
    "\n",
    "If you click on it, you can see that for now it is empty.\n",
    "\n",
    "After having created the bucket to store the logs, we are going to create a new user with only the permission\n",
    "\n",
    "to read and write to that bucket.\n",
    "\n",
    "Indeed, it is a best practice to do this to avoid potential security issues.\n",
    "\n",
    "So, click on “Services”, type “IAM” and select the service. Then click on “Users”\n",
    "\n",
    "and “Add User”. We need to define a username,\n",
    "\n",
    "let’s say “airflow-log-s3”.\n",
    "\n",
    "We select “Programmatic access” since we will need an access key ID and a secret access key in order\n",
    "\n",
    "to create the connection to AWS S3 from Airflow. Click on “Next”. Select “Attach existing policies\n",
    "\n",
    "directly” and click on “Create policy”.\n",
    "\n",
    "Here, we are going to create a policy to only give the read write permission for the bucket we created\n",
    "\n",
    "previously. Select “Choose a service” and select “S3”. Then in “Actions” under “Access Level”\n",
    "\n",
    "and “List”, choose “ListBucket”. In “Read”, choose “GetOject”. In “Write”,\n",
    "\n",
    "select “PutObject”, “DeleteObject”, “ReplicateObject” and “RestoreObject”. Now the actions are set, in\n",
    "\n",
    "“Resources”, click on the warnings.\n",
    "\n",
    "From there we have to specify the resources that the user is allowed to interact with.\n",
    "\n",
    "First, we restrict the access to only the bucket we created by clicking on “Add ARN”. Here, type the\n",
    "\n",
    "name you defined for the bucket. In my case it’s “marcl-airflow-logs”.\n",
    "\n",
    "Check that you didn’t make any mistakes and click on “Add”.\n",
    "\n",
    "Okay.\n",
    "\n",
    "We do the same for the objects.\n",
    "\n",
    "Type the same bucket name,\n",
    "\n",
    "“marcl-airflow-logs” in my case\n",
    "\n",
    "and select any for the object name.\n",
    "\n",
    "Click on “Add”.\n",
    "\n",
    "Perfect.\n",
    "\n",
    "We can review the policy.\n",
    "\n",
    "We need to give a name to the policy, let’s say “ReadWriteS3AirflowLogs”.\n",
    "\n",
    "And finally click on “Create policy”.\n",
    "\n",
    "Okay back to the User view, refresh the policies by clicking here\n",
    "\n",
    "and select “ReadWriteS3AirflowLogs”. Click on “Next”, “Next” again and “Create User”.\n",
    "\n",
    "Perfect,\n",
    "\n",
    "the user has been well created.\n",
    "\n",
    "Here, we got the access key ID and secret access key that we gonna use in Airflow to connect\n",
    "\n",
    "to S3.\n",
    "\n",
    "So, download the credentials right now, as you would not be able to see this page again and save them\n",
    "\n",
    "somewhere safe.\n",
    "\n",
    "Click on “Close”. So we done with AWS,\n",
    "\n",
    "let’s move to the Airflow side. In you terminal, check that you are under the folder airflow-materials/\n",
    "\n",
    "airflow-section-8 and start the docker containers\n",
    "\n",
    "with start.sh.\n",
    "\n",
    "As you can see, docker is building a new docker image of Airflow.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because in order to store your logs into AWS S3, you have to add the package S3 along with\n",
    "\n",
    "the install of Airflow. To be more concrete,\n",
    "\n",
    "go to your code editor, check that you are under the folder airflow-materials/airflow-section-\n",
    "\n",
    "8 and open the dockerfile in the folder Docker.\n",
    "\n",
    "Here, you have the different commands to setup the docker container running Airflow.\n",
    "\n",
    "If you take a look at the line where Airflow is installed with pip, you can see here, the package S3 as\n",
    "\n",
    "well the other usual packages such as crypto, celery, postgres and so on.\n",
    "\n",
    "So if you want to access S3 with Airflow, this package must be installed.\n",
    "\n",
    "Alright. Back to the terminal,\n",
    "\n",
    "I’m gonna pause the video right now, and I will come back when the build is done. Ok,\n",
    "\n",
    "the build is done, airflow should be running,\n",
    "\n",
    "let’s type “docker\n",
    "\n",
    "ps”.\n",
    "\n",
    "and the containers are running as expected.\n",
    "\n",
    "Perfect.\n",
    "\n",
    "Now in your web browser, open a new tab\n",
    "\n",
    "and type localhost:\n",
    "\n",
    "8080.\n",
    "\n",
    "Enter. From the Airflow\n",
    "\n",
    "UI, click on “Admin”, “Connection” and “Create”.\n",
    "\n",
    "Here, we are going to setup the connection we need to access the S3 bucket from Airflow.\n",
    "\n",
    "Let's type “AWSS3LogStorage” for the name.\n",
    "\n",
    "Then select s3 for the connection type.\n",
    "\n",
    "Finally, in the extra field type the following json value, {\"aws_access_key_id\": \"the access key\", \"aws_secret_access_key\": \"the secret key\"}.\n",
    "\n",
    "Notice that for each access key you should input your credentials from the csv file you downloaded\n",
    "\n",
    "earlier.\n",
    "\n",
    "Like that.\n",
    "\n",
    "Okay.\n",
    "\n",
    "Click on “Create”. Perfect\n",
    "\n",
    "the connection has been successfully created. Now, from your code editor,\n",
    "\n",
    "open the file airflow.cfg.\n",
    "\n",
    "In order to use an external storage for your logs, three parameters must be defined.\n",
    "\n",
    "Remote_logging, remote_log_conn_id and remote_\n",
    "\n",
    "base_log_folder.\n",
    "\n",
    "Let's start by the first one.\n",
    "\n",
    "Remote_logging\n",
    "\n",
    "when set to true allows Airflow to write and read logs from a remote location. So change the value from\n",
    "\n",
    "False to True. Then, remote_log_conn_id corresponds to the connection\n",
    "\n",
    "we just created to connect to AWS S3.\n",
    "\n",
    "So here, we have to put the connection id AWSS3LogStorage.\n",
    "\n",
    "Finally, remote_base_log_folder indicates the folder where the logs\n",
    "\n",
    "are going to be stored and read from your remote storage.\n",
    "\n",
    "So type s3:// your bucket name which is marcl-airflow-logs in my case\n",
    "\n",
    "and the folder where we want to save the logs which is /airflow-logs.\n",
    "\n",
    "At the end,\n",
    "\n",
    "you should have the same line as mine except for the name of the bucket, where you have to put own.\n",
    "\n",
    "Okay save the file.\n",
    "\n",
    "Now, if you go back to the AWS S3 dashboard,\n",
    "\n",
    "you should find your bucket here and if you click on it, it should be empty.\n",
    "\n",
    "All right,\n",
    "\n",
    "it’s time to see if everything works as expected. From your terminal,\n",
    "\n",
    "execute the command ./restart.sh so that the modifications we made are applied.\n",
    "\n",
    "Ok, type docker\n",
    "\n",
    "ps.\n",
    "\n",
    "Enter. Great, now\n",
    "\n",
    "type docker logs -f with the container id of the worker where tasks of DAGs are going to be executed.\n",
    "\n",
    "Enter. The purpose of this command is to show the connections that will be made to your AWS\n",
    "\n",
    "S3 bucket by airflow in order to read and write the logs.\n",
    "\n",
    "So in your web browser, go to the Airflow UI and click on “DAGs”. From there,\n",
    "\n",
    "turn on the toggle of the DAG logger_dag to schedule it, and wait for the DAGRun to finish.\n",
    "\n",
    "This DAG has only two tasks, t1 doing nothing and t2 printing out a message to the standard output.\n",
    "\n",
    "All right,\n",
    "\n",
    "the DAGRun is finished,\n",
    "\n",
    "click on the DAG, then “graph view”. Let’s click on “t1” and “view log”.\n",
    "\n",
    "From there\n",
    "\n",
    "pay attention to the first line here indicating that the current logs you are reading, have been fetched\n",
    "\n",
    "remotely from the following file stored in your S3 bucket.\n",
    "\n",
    "Indeed, if you go back to the S3 dashboard and click on this icon to refresh the bucket, you can see\n",
    "\n",
    "a new folder called airflow-logs.\n",
    "\n",
    "If you click on it, you obtain another folder with the name of the DAG logger_dag. Then, in it\n",
    "\n",
    "and you have one folder for each task.\n",
    "\n",
    "Click on “t1”, the last date, and 1.log. “Open”. And it corresponds to the logs displayed from the Airflow UI.\n",
    "\n",
    "Well done.\n",
    "\n",
    "You are now able to store and read the logs produced by Airflow in AWS S3. Before moving forward,\n",
    "\n",
    "go back to your terminal, and execute the script\n",
    "\n",
    "./stop.sh.\n",
    "\n",
    "Finally, in your code editor, remove the values of the variables remote_log_\n",
    "\n",
    "conn_id and remote_base_log_folder\n",
    "\n",
    "then set the value of remote_logging to False.\n",
    "\n",
    "Save the file and we are done.\n",
    "\n",
    "Perfect.\n",
    "\n",
    "Let's take a quick break and see you for the next video.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
