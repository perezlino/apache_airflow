{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 - A bit more about the architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si te preguntas cómo funciona todo, permíteme darte algunas explicaciones rápidas pero importantes.\n",
    "\n",
    "En los próximos vídeos vas a ejecutar el SparkSubmitOperator, el HiveOperator, el BashOperator para interactuar con HDFS, etc.\n",
    "\n",
    "Para interactuar con esas herramientas, los binarios correspondientes deben estar instalados donde está instalado Airflow.\n",
    "\n",
    "Este proyecto se ejecuta en Docker y la imagen Docker utilizada para ejecutar Airflow se construye a partir de otras imágenes Docker múltiples.\n",
    "\n",
    "Openjdk image -> Hadoop -> Hive -> Spark -> Airflow\n",
    "\n",
    "Por eso se puede interactuar con Spark en el contenedor docker de Airflow. Porque la imagen docker de Airflow se construye a partir de la imagen docker de Spark que se construye a partir de la imagen Docker de Hive y así sucesivamente."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
