{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4 - [Práctica] Define your DAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, es el momento de iniciar el proyecto y crear tu primer data pipeline. Para crear un data pipeline en Airflow, normalmente lo que vas a hacer es crear primero un objeto DAG correspondiente a tu data pipeline. A continuación, dentro de este objeto DAG, vas a implementar las diferentes tareas que quieres añadir a tu DAG, a tu data pipeline. Por último, pero no menos importante, vas a especificar las dependencias entre tus tareas, para decir que estas tareas deben ser ejecutadas primero y luego la otra. \n",
    "\n",
    "Ahora, en esta clase, vas a crear el objeto DAG. Así que este es el primer paso. Antes de hacerlo, me gustaría decirte algo primero. Si abres la carpeta 'airflow-section-3' y abres la carpeta, 'dag_solutions', puedes ver todas las versiones del DAG que vas a crear según el vídeo que estás viendo. Por ejemplo, ahora mismo, la solución para el DAG es 'forex_data_pipeline_v_1.py'. \n",
    "\n",
    "Además, la segunda cosa que me gustaría decirte, si has abierto la carpeta 'mnt' > 'airflow', la carpeta 'dags' es donde vas a crear tu DAG, ¿por qué? Recuerda que esta carpeta está sincronizada con la misma carpeta pero dentro del contenedor docker correspondiente a Airflow. \n",
    "\n",
    "Así que esto es súper importante. Cada vez que añades un DAG, lo añades en la carpeta 'dags', eso es todo.\n",
    "\n",
    "Vamos a crear el DAG. Así que en la carpeta 'dags', crear un nuevo archivo y vamos a llamarlo 'forex_data_pipeline.py'. Cada vez que quieras crear un data pipeline y en airflow tienes que crear un archivo python. Entonces en ese archivo python, la primera línea que hay que añadir es la siguiente: 'from airflow import DAG', para importar el objeto DAG. Y recuerda, el objeto DAG es en realidad tu data pipeline, así que necesitas instanciar un objeto DAG.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/MpGmTLKm/a560.png\"></center>\n",
    "\n",
    "Una vez que tienes el objeto DAG, puedes escribir 'with', la declaración DAG para instanciar el objeto DAG y 'as dag'. Además tienes que añadir algunos parámetros que vas a ver ahora mismo:\n",
    "\n",
    "**`\"forex_data_pipeline\"`** : **dag_id**\n",
    "\n",
    "**`default_args = { … }`**  => **es un diccionario donde vas a especificar los atributos comunes de las tareas** \n",
    "\n",
    "El \"**`owner`**\" de todas las tareas que vas a implementar en ese DAG es Airflow.\n",
    "\n",
    "**`email_on_failure`** : puedes especificar si quieres recibir un correo electrónico en caso de fallo. Si una tarea falla. \n",
    "\n",
    "**`email_on_retry`** : puedes especificar si quieres un correo electrónico, cada vez que una tarea es reintentada por Airflow.\n",
    "\n",
    "**`email`** : indico el correo al cual serán enviadas las alertas\n",
    "\n",
    "**`retry`** : si escribe 'retry' y pone uno, eso significa que si su tarea falla, esa tarea será reintentada por Airflow al menos una vez antes de terminar con el estado \"failure\".\n",
    "\n",
    "**`retry_delay`** : Aquí se especifica un objeto timedelta con minutos iguales a cinco. Así que estás diciendo, quiero reintentar mi tarea una vez, pero antes de reintentar mi tarea, quiero esperar cinco minutos.\n",
    "\n",
    "**`catchup = false`** => Si envía el parámetro 'catchup' a 'false', evitará que se ejecuten todos los Dag runs 'non triggered' entre la fecha actual (current date) y la start date.\n",
    "\n",
    "Ten en cuenta que los '**`default_args`**' se aplican a tus operators, a tus tareas. Así que esos argumentos no son argumentos para el objeto DAG. Son para tu tarea. Se aplican a sus tareas.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/QCspr4y5/a561.png\"></center>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
