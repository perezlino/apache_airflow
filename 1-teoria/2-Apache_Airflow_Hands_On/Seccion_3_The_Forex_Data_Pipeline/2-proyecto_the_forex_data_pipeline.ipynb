{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 - Proyecto: The Forex Data Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es hora de que crees tu primer data pipeline de la A a la Z. A partir de ahora, lo que vas a aprender será extremadamente útil para el resto de tu viaje en Airflow.\n",
    "\n",
    "Hay algunas cosas que tienes que saber. En primer lugar, ¿cuál es la regla de este data pipeline? ¿Qué es lo que va a lograr? Bueno, el objetivo del Forex data pipeline no es predecir el futuro de los precios de las divisas. En mi opinión, no es realmente posible. Pero sí va a procesar datos de Forex. ¿Qué son los datos forex? Bueno, déjame mostrarte esto. Estos son los datos de Forex. Y más específicamente, es el EURO contra el USD, como puedes ver aquí, ¿qué significa? Bueno, el USD o en realidad cualquier moneda fluctúa contra otra moneda. En este caso concreto, el EURO fluctúa frente al USD. Ahora mismo, un EURO es igual a 1,188 dólares. Y eso es lo que vas a manipular en el Forex data pipeline.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/Pq9hC0bG/a557.png\"></center>\n",
    "\n",
    "No es tan complicado como esto, pero usted va a obtener algunos valores de las monedas EURO y luego procesar esas monedas EURO. Eso es lo único que necesitas saber sobre los datos forex.\n",
    "\n",
    "Ahora, ¿cuáles son los diferentes pasos del data pipeline de Forex? El proyecto se divide en ocho pasos diferentes u ocho tareas diferentes. \n",
    "\n",
    "-\tY la **`primera tarea`** será comprobar si los pesos forex (forex weights) (las divisas) están disponibles o no. Usted va a llegar a una URL específica y entonces verificará que la URL es accesible o no.\n",
    "\n",
    "-\tLuego, en la **`segunda tarea`**, podrá verificar si el archivo donde se encuentra la moneda que desea obtener de la URL especificada, está disponible o no.\n",
    "\n",
    "-\tEn el **`tercer paso`**, descargarás los valores de las divisas (forex rates) ejecutando una función de python. Y aquí es donde usted aprenderá cómo hacerlo en Airflow.\n",
    "\n",
    "-\tEn el **`cuarto paso`**, guardará los valores de las divisas (forex rates) en HDFS. HDFS es un sistema de archivos distribuido, por lo que si usted tiene terabytes de datos, puede utilizar HDFS con el fin de almacenar sus datos. Eso es lo que vas a descubrir aquí.\n",
    "\n",
    "-\tEn el **`quinto paso`**, crearás una tabla HIVE para interactuar con los valores de las divisas (forex rates) que están almacenados en tu HDFS. \n",
    "\n",
    "-\tEn el **`sexto paso`**, procesarás los valores de las divisas (forex rates) con Spark. Aquí aprenderás cómo enviar un Spark Job, cómo lanzar un Spark Job desde tu data pipeline en airflow.\n",
    "\n",
    "-\tY **`luego tienes dos tareas más`**. La primera es enviar un correo electrónico para notificar que el data pipeline está hecho, que todas las tareas se han ejecutado. Y por último, enviarás una \"notificación de Slack\" para avisar a tu equipo de que el data pipeline está terminado.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/cH3WwTTv/a558.png\"></center>\n",
    "\n",
    "Así que como puedes ver, vas a aprender mucho durante ese proyecto. Muchos pasos, muchas tareas que implementar. Pero antes de pasar al código, déjame darte la arquitectura en la que tu data pipeline se va a ejecutar.\n",
    "\n",
    "Esta es la hermosa arquitectura en la que tu data pipeline va a funcionar.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/Gt1CQYSq/a559.png\"></center>\n",
    "\n",
    "Y en realidad vas a obtener todas esas herramientas, esta arquitectura, con un comando muy simple, gracias a Docker Compose. Pero déjame darte una explicación rápida sobre ello. \n",
    "\n",
    "**`Así que primero puedes dividir esta arquitectura en dos cosas: el front end y el back end`**. Empezando por el back end, tienes el '**HDFS**' y el HDFS es un sistema de archivos distribuido donde puedes almacenar todos los datos que quieras. En ese caso, tienes el 'Namenode' y el 'Datanode'. Y aquí es donde se almacenarán tus divisas.\n",
    "\n",
    "Luego tienes '`**`Spark`**`' para procesar las divisas. Spark es un framework de procesamiento para procesar datos a escala. Si no conoces esa herramienta, te aconsejo encarecidamente que le eches un vistazo.\n",
    "\n",
    "Luego tienes '**`Hive`**'. Hive te permite interactuar con los archivos que tienes almacenados en tu HDFS, pero utilizando el tipo de sintaxis de SQL. Así que usarás SQL para consultar los datos que tienes en el HDFS y tendrás algunas tablas Hive correspondientes a los archivos que tienes en tu HDFS.\n",
    "\n",
    "Entonces tienes una base de datos **`Postgres`** y la base de datos Postgres es utilizada por Hive, pero no sólo por '**`Adminer`**' y Adminer es una pequeña herramienta para interactuar con tu base de datos Postgres con facilidad.\n",
    "\n",
    "Luego tienes '**`Hue`**' y Hue te permite tener un bonito dashboard para comprobar los datos que tienes en Hive así como en HDFS.\n",
    "\n",
    "Finalmente tienes 'Airflow'. Obviamente Airflow es el orquestador. Donde vas a crear tu data pipeline y tu data pipeline va a interactuar con todos los componentes que puedes ver aquí a través de los operators."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
