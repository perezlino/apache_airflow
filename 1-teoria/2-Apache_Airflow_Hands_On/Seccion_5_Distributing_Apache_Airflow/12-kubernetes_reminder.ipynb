{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.12 - Kubernetes reminder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen the SequentialExecutor, the LocalExecutor and the CeleryExecutor.  Each of them with their own specificity and complexity. In the following videos, we are moving to the  next level by discovering the Kubernetes Executor.  But before doing so, let me give a quick reminder about Kubernetes.  So what is Kubernetes? Kubernetes is an open-source platform for managing containerized applications.  Founded by Google in 2014,  it orchestrates computing, networking and storage infrastructure while offering nice features such as  autoscaling, load balancing, monitoring and so on.  Basically it automates sysadmin tasks. From a developer perspective, it reduces the time and effort needed  to deploy applications and allows zero-downtime deployments with rolling updates by default.  A rolling update is when starting containers with the new version are waiting until they become healthy,  and then shut down the old ones.  Kubernetes provides facilities for implementing continuous deployment practices such as canary deployments  and blue-green deployments.  The first one is when you gradually rolling out updates one server at a time to catch problems early.  The second one is when you spin up a new version of the system in parallel and switch traffic over to it  once it is fully up and running. With kubernetes  you can either scale up or down your application according to the demand.  This flexibility is a huge advantage as it avoids wasting resources when your application is not used  anymore. By default,  Kubernetes has redundancy and failover built in so your application will be more reliable and resilient.  They are much more things to say about Kubernetes as it is a very powerful tool but you have seen some  very important points.  Now let's take a look at the drawbacks.  If you want to set up your own kubernetes cluster, this is pretty simple to do.  But if you want to set up a kubernetes cluster that is ready for production,  that's another story.  . They are many questions that you will need to consider such as: Is my cluster set up securely? Is the  data in my cluster properly backed up?  What happens if the master node goes down? Is the control plane  highly available and much more. Also, when you make changes or upgrades to your Kubernetes cluster  you will need to consider the impact of high availability, security, and so on. Kubenetes is still in  rapid development and new features are being released all the time.  It can be very hard to maintain your cluster if your team is not ready for it.  There is a significant investment of time and energy involved both in learning how to manage your own cluster  properly and actually doing it from day to day.  That’s why it is widely recommended to go with Managed Kubernetes Services such as Google Kubernetes  Engine or Amazon Elastic Container Service for Kubernetes.  That being said, in our case we are going to set up a cluster on promise, but don’t worry as it is  not a Devops course, you won’t need to dive into Kubernetes in order to use the Kubernetes Executor  of Airflow.  Let’s take a look at the components of a kubernetes cluster.  Here, you have a master node and worker nodes.  Let’s begin with the master node.  The cluster brain is called the control plane running inside the master node, and it runs all the tasks  required for kubernetes to do its job. Scheduling containers, managing services, serving API requests  and so on.  Several components make the control plane. The kube-apiserver which is the frontend server for the  control plane, handling API requests.  The kube-scheduler decides where to run newly created Pods.  We will see the Kubernetes objects in a minute.  Next there is the kube-controller-manager which is responsible for running resource controllers like Deployments.  Finally, there is “etcd” which is the database where Kubernetes stores all of its information such as  which node is available, what resources exist on the cluster, and so on.  After the master node we have the worker nodes where the workload is run.  Each worker in a Kubernetes cluster runs these components. The kubelet, which is responsible for driving  the container runtime to start workloads that are scheduled on the node and monitoring their status. And  there is the kube-proxy, in charge of routing requests between Pods on different nodes, and between Pods  and the Internet.  Basically, the difference between master nodes and worker nodes is that master nodes don’t usually  run user workloads except for very small clusters.  Ok so we have the components of a Kubernetes cluster,  let’s see some important Kubernetes objects that you may have to deal with.  Here is a schema of how the different objects of Kubernetes interact with each other.  Objects are yaml files describing a resource in Kubernetes.  Let’s begin with the most basic object which is the Pod. A pod is a group of one or more containers  like Docker containers, with shared storage and network with a specification for how to run the containers.  Then, we have the Replica Set. A Replica Set ensures that a specified number of pod replicas are running at  any given time.  You know the world's replica set makes sure that a pod or a set of pods is always up and available.  It helps you to define how many pods are available.  For example, if you set replica to 3, then if one pod die, the Replica Set creates a Pod to make it  3. Pods and replicas are defined into a Deployment object.  Basically, you describe a desired state in a Deployment object, and the Deployment controller changes  the actual state to the desired state.  Think of it as the blueprint of your application indicating what to start and how and if the blueprint  isn't respected anymore because of a failure.  For example, the Deployment controller will make sure that your application comes back to a normal state.  Okay your deployment object or application will interact with other objects such as Services, Storage  places and Persistent Volume Claims.  A service is an abstraction which defines a logical set of Pods and a policy by which to access them.  Without a service,  you would need to access a Pod by it’s IP address which is not suitable as it may goes down for any  reason.  Think of a service as a gateway between you and your Pods.  Then we have the Storage Class. A Storage class provides a way for administrators to describe the “classes”  of storage they offer.  It represents a Persistent Volume such as NFS, AWS EBS and so on.  Finally, Persistent Volume Claims are objects that request storage resources, persistent volumes, from  your cluster.  Then a Pod will be able to interact with a Persistent Volume throughout the Persistent Volume Claim.  I made the following slide to help to remember what I have just said, but don’t worry, that’s not  required for using the Kubernetes Executor in the course.  Nonetheless, I strongly think that it is important for a data engineer to understand the architecture on  which they take pipelines are running.  Alright, I hope you enjoyed this quick reminder of Kubernetes,  now it’s time to take a look at the Kubernetes Executor.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
