{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 - Define your DAG, the right way**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empecemos con la pregunta más importante cómo definir tus DAGs y sé lo que estás pensando ahora mismo, sí sé cómo definir mis DAGs es realmente fácil así que cuál es el propósito de este vídeo. Bueno, no voy a mostrarte cómo definir tu DAG porque es bastante fácil, pero voy a mostrarte cuáles son los argumentos que debes especificar siempre que se instancie un DAG. Y hay algunos detalles que puede que no sean XXX. Así que si estás listo vamos a sumergirnos en el código. OK, estás listo para agregar tu DAG así que el primer paso es crear el archivo Python así que ve a la carpeta dags y crea un nuevo archivo y llamémoslo 'my_dag.py'. Ahora la primera cosa que necesitas saber es que el scheduler tratará de analizar este archivo sólo si contiene la palabra 'dag' o la palabra 'airflow'. Bien, eso significa que si pones 'airflow' aquí o dag, el scheduler intentará analizar este archivo para comprobar si es efectivamente un DAG o no.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/655j6T39/a1078.png\"></center>\n",
    "\n",
    "Esto es importante de recordar, porque si por ejemplo generas tus DAGs dinámicamente y no pusiste la palabra 'dag' o 'airflow' en ese archivo, el scheduler no analizará tu archivo y por lo tanto no generará tus DAGs dinámicamente. Por eso si importas el objeto dag con 'from airflow import DAG' ahora el scheduler es consciente de que este archivo es realmente un DAG. Así que vamos a proceder. Puedes modificar esto cambiando la configuración 'DAG_DISCOVERY_SAFE_MODE' en la sección 'core' del archivo de configuración de Airflow y así si defines esa configuración en 'False' por defecto el scheduler intentará pasar todos los archivos que tengas en la carpeta dags. Obviamente no te recomiendo que hagas eso pero es bueno saberlo. \n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/VLG7J1rX/a1079.png\"></center>\n",
    "\n",
    "Una última cosa importante, puedes añadir un archivo '.airflowignore' en tu carpeta dags. Y en ese archivo puedes poner algunos archivos, extensiones o carpetas que no quieras analizar, por ejemplo, si tienes una carpeta '/sql' y no tiene sentido que el scheduler analice esta carpeta, puedes ponerla en ese archivo. Básicamente, funciona exactamente igual que con el archivo '.gitignore' y acelerará el proceso de análisis de sus DAGs si tiene una tonelada de archivos en su carpeta dags. Así que esto es realmente una buena práctica. Dicho esto vamos a eliminar ese archivo, porque ahora mismo sólo tenemos como dos dags hasta ahora y vamos a centrarnos en el archivo dag. Así que aquí una vez que has importado el objeto dag vamos a instanciar el objeto dag y para hacer eso la forma que yo prefiero es mediante el uso de un 'context manager', así que escribes 'with DAG' y defines los argumentos que necesitas, 'as dag' y luego abajo defines las tareas por ejemplo el 'DummyOperator' y así sucesivamente.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/pLms87z4/a1080.png\"></center>\n",
    "\n",
    "También hay otra forma que es creando una 'variable dag' y luego igualando al objeto dag pero no me gusta mucho esta forma. ¿Por qué? Porque tendrás que especificar para todas tus tareas, que tus tareas pertenecen a ese objeto dag. Así que básicamente, con el DummyOperator, por ejemplo, tomarás el DummyOperator y luego 'dag=dag' y tendrás que hacer eso para todas tus tareas, lo cual es bastante redundante y además eso no te obliga a poner todas tus tareas en el otro objeto dag. Así que usted puede tener algunas tareas en otros archivos y así sucesivamente, entonces en algún momento será realmente difícil de mantener su dag. Por eso, en mi opinión, es mucho mejor usar un 'Context Manager (with DAG(...) )' y poner todas tus tareas bajo él para que quede claro y no tengas que especificar dag=dag para cada una de tus tareas.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/Z5XVLcLR/a1081.png\"></center>\n",
    "\n",
    "Dicho esto, vamos a ver los argumentos que en mi opinión son súper importantes de especificar. Y el primero es obviamente el '**`dag_id`**'. Así que el 'dag_id' es realmente el identificador único de su DAG y usted tiene que asegurarse de que sea único a través de todos sus dags. Así que, por ejemplo, aquí podrías poner algo como 'my_dag' e incluso puedes quitar 'dag_id =' porque el primer argumento es 'dag_id' por defecto.\n",
    "\n",
    "Ahora bien, si tienes otro DAG con el mismo 'dag_id', no obtendrás ningún error, sino que aparecerá un DAG u otro de forma aleatoria en la UI. Así que es realmente importante que te asegures de que cada uno de tus DAGs tenga un único 'dag_id'. Repito si tienes dos DAGs con el mismo 'dag_id' no recibes ningún error y no sabes que DAG se mostrará en la UI, aleatoriamente obtendrás un DAG u el otro. Así que una vez que tengas tu 'dag_id' listo, el siguiente argumento es la '**`description`**' y esta es bastante básica pero es realmente una buena práctica poner una descripción para cada uno de tus DAG, tienes que explicar cuál es el objetivo de ese DAG y serás feliz porque si terminas con como cientos de diferentes DAGs bien sólo mirando en la UI sabrás realmente lo que este DAG hace y será mucho más rápido que tener que echar un vistazo en el código. Así que por ejemplo aquí se podría decir \"DAG encargado de procesar los datos de los clientes\". Luego el siguiente argumento es el '**`start_date`**'. El start_date no es obligatorio, no tienes que especificar tu start_date para tu objeto dag. Si no lo haces no recibirás ningún error y es opcional. Pero de hecho si creas una tarea, así por ejemplo, un DummyOperator y no especificaste ninguna start_date recibirás un error diciendo, hey, esta tarea no tiene ningún inicio así que tienes que ponerle uno. Así que por eso aunque por defecto no tengas que definir el start_date dentro de tu objeto dag, tus tareas necesitan el start_date por lo que debes especificar uno. Así que ya sea dentro de su objeto dag o ya sea como argumentos por defecto.\n",
    "\n",
    "En mi opinión, coloque start_date dentro del objeto dag, por ejemplo, escriba start_date igual al objeto datatime y, por ejemplo, el 1 de enero de 2021. Entonces, el start_date se aplicaría a todas sus tareas. Lo que necesita saber sobre start_date es que define la fecha en la que su DAG comienza a programarse (schedule it), por ejemplo, si start_date es igual al 1 de enero de 2021 y la fecha actual es el 5 de enero de 2021, entonces su DAG se programará a partir de esa fecha. Entonces, si su start_date es hace dos años, de forma predeterminada terminará con muchas ejecuciones de sus  DAG (DAG Runs) ejecutándose al mismo tiempo y hay un parámetro que debe configurar para evitar esto. Luego, por último, pero no menos importante, cada tarea puede tener una start_date diferente. Entonces, en realidad, cada operador (cada tarea) puede tener su propia start_date. No me preguntes por qué, no tengo idea de en qué o para qué casos de uso podrías necesitar eso, pero es por eso que normalmente verás en los diferentes argumentos la start_date definida porque desea asegurarse de que todos sus operadores (todas sus tareas) compartan la misma fecha de inicio. Entonces, solo especifique la start_date dentro de su objeto dag es mucho más simple.\n",
    "\n",
    "Agreguemos el 'datetime object' y escribe 'from datetime import datetime'. Luego, el siguiente argumento que es bastante útil es el '**`schedule_interval`**'. El schedule_interval define la frecuencia con la que se activa su DAG, por ejemplo, \"todos los días\", puede usar @daily o \"cada semana\", puede usar @weekly o puede especificar una \"expresión cron\" como cada 10 minutos (* /10 * * * *) o puede definir un 'objeto timedelta' como \"timedelta(minutes=5)\", cada cinco minutos. Por lo tanto, el schedule_interval no es como si no tuviera que especificarlo, por defecto, el valor de intervalo de programación es \"un día\" con el objeto timedelta, pero obviamente es bastante útil especificar ese argumento; de lo contrario, no podrá activar su DAG cuando quieras activarlo. Por lo tanto, siempre especifique su schedule_interval. Y una cosa que debe saber es la diferencia entre una \"expresión cron\" y un objeto \"timedelta\" para el intervalo de programación.\n",
    "\n",
    "Entonces, una vez que tenga su schedule_interval, vamos a @daily. Otro argumento bastante útil lo es '**`dagrun_timeout`**'. Este argumento indica que si su DAG tarda más de, por ejemplo, 10 minutos en completarse, entonces falla. Y, de forma predeterminada, no hay un tiempo de espera (timeout) predeterminado para su DAG. ¿Y por qué es un problema para ti? Porque si su DAG tarda más de 10 minutos en completarse y su schedule_interval está establecido en 10 minutos, su primer DAG seguirá ejecutándose y se activará el siguiente DAGRun, por lo que terminará con dos DAG ejecutándose incluso si el primero todavía no he terminado, y es posible que no quieras eso. Porque si su DAG tarda más de 10 minutos y espera que su DAG se complete en 10 minutos, eso significa que hay un problema con su DAG, por lo que debe solucionar el problema. Por lo tanto, siempre especifique un 'dagrun_timeout' que corresponda aproximadamente a la herramienta de programación que tiene, por ejemplo, si tiene un schedule_interval establecido en 10 minutos, podría ser interesante establecer un dagrun_timeout en 12 minutos. Entonces, especifiquemos timedelta (minutes = 10). Obviamente, necesita importar el objeto timedelta. Así que siempre haga eso, tenga en cuenta que Airflow no omitirá el próximo DAGRun, sino que activará el próximo DAGRun incluso si la actual aún se está ejecutando. Otro argumento que me encanta es el argumento '**`tags`**' y este es bastante básico pero bastante útil, por ejemplo, digamos que tiene diferentes equipos trabajando en su DAG, por ejemplo, el equipo de DataScience o el equipo de DataEngineer, puede especificar etiquetas para este DAG y luego usará esas etiquetas para filtrar sus DAG en la interfaz de usuario. Entonces, por ejemplo, digamos que pertenecen al equipo \"data_science\" y además se pueden agregar muchas etiquetas a su DAG.\n",
    "\n",
    "El último argumento para agregar es el argumento '**`catchup`**' y este argumento se encarga de decir si desea activar (trigger) todos los DAGRuns no activados automáticamente, por ejemplo, supongamos que cometió un error en su DAG y ha pausado su DAG durante cinco días, si vuelve a programar (schedule) su DAG, Airflow ejecutará de forma predeterminada todos los DAGRuns no activados durante esos cinco días. Si no desea hacerlo automáticamente, debe establecer el parámetro de catchup en 'False' y, en mi opinión, es una buena práctica establecer siempre ese parámetro en 'False'. ¿Por qué? porque si, por ejemplo, tiene una start_date establecida hace 10 días o por qué no hace un año, terminaría con una tonelada de DAGRuns ejecutándose al mismo tiempo automáticamente. Si, por ejemplo, tiene un schedule_interval muy ajustado y detuvo su DAG durante 2 horas, tal vez también termine con una tonelada de DAGRuns. Por lo tanto, siempre es mejor tener un control total sobre su DAG, por lo que, como práctica recomendada, establezca este parámetro en 'False'. Una cosa a tener en cuenta es que puede establecer este parámetro en 'False' automáticamente modificando la configuración en el archivo de configuración de Airflow \"catchup_by_default\". Y una última cosa que aún puede rellenar su DAG, aún puede ejecutar los DAGRuns no activados o los DAGRuns anteriores ya activados con la interfaz de línea de comando usando \" airflow dags backfill\". Entonces en este punto tienes todos los argumentos que son en mi opinión los más importantes y los que siempre debes definir.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/R0fGpHPT/a1082.png\"></center>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
