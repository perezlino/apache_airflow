{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1 - ¿Que es Airflow?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow es el mejor orquestador para ejecutar sus tareas de la manera correcta, en el orden correcto y en el momento adecuado.\n",
    "\n",
    "Por lo general, se utilizará MySQL o Postgres, pero no es más que una base de datos que se conectará con su Airflow en su lugar, para que todos los metadatos añadidos a sus tareas, a cada instancia, sean almacenados.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/tTyF9Dwx/a1.png\"></center>\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`Core components`**\n",
    "\n",
    "Así que ten en cuenta esta diferencia, el Executor define cómo se van a ejecutar tus tareas, en qué sistema y un Worker es el proceso donde se ejecuta efectivamente tu tarea.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/ZRcpnpzD/a2.png\"></center>\n",
    "\n",
    "- El **`web server`** es un servidor web de flask Python que permite acceder a la interfaz de usuario.\n",
    "- El **`scheduler`** es muy crítico porque es el encargado de programar tus tareas, tus data pipelines. Así que debes asegurarte de que tu scheduler funcione, de lo contrario, nada funcionará.\n",
    "- La **`metadatabase`** no es más que una base de datos compatible con la alquimia SQL. Por ejemplo, Postgres, MySQL, Oracle DB, SQL server, etc. En esta base de datos, tendrás metadatos relacionados con tus data pipelines, tus tasks, usuarios de airflow, Jobs, variables, connections, etc., que serán almacenados.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, tiene un concepto llamado **`Executor`** y **`un Executor define cómo y en qué sistema se ejecutan tus tasks`**.\n",
    "\n",
    "Por ejemplo, si tienes un Kubernetes cluster, quieres ejecutar tus tasks en este Kubernetes cluster, utilizarás el **`KubernetesExecutor`**.\n",
    "\n",
    "Si quieres ejecutar tus tasks en un **`Celery cluster`**, **`Celery es un framework de Python para ejecutar múltiples tasks en múltiples máquinas`**, utilizarás el **`CeleryExecutor`**. Ten en cuenta que el executor no ejecuta ninguna task.\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un queue tus tasks serán empujadas (pushed) en ella con el fin de ejecutarlas en el orden correcto, y **`el worker es donde sus  tasks son efectivamente ejecutadas`**.\n",
    "\n",
    "**`Ahora, ten en cuenta que con cualquier executor, siempre tienes un queue detrás de cada executor con el fin de ejecutar tus tasks en el orden correcto`**.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/8czW6c1k/a3.png\"></center>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y si no tienes un worker, tienes algunos subprocesos donde se ejecutan tus tasks o algunos PODs si usas Kubernetes.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/gjthqpJV/a4.png\"></center>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`DAG (Directed Acyclic Graph)`**\n",
    "\n",
    "Básicamente en Airflow es un data pipeline. Su data pipeline se representa como un grafo.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/sfT5RYcM/a5.png\"></center>\n",
    "\n",
    "**`¿Qué es un DAG?`**\n",
    "\n",
    "Un DAG significa directed acyclic graph, y no es más que un gráfico con nodos, aristas dirigidas (directed edges) y sin ciclos.\n",
    "___\n",
    "\n",
    "Tienes tus nodos T1, T2, T3 y T4 que corresponden a tus tasks y tienes las directed AG's o las dependencias de tus tareas como muestran las flechas. También se tiene directed dependencies o edges. Puedes ver que T4 depende de T1, T2 y T3 y no hay ningún bucle. Es un DAG.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/QMZTq0Xy/a6.png\"></center>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`Operator`**\n",
    "\n",
    "Puedes pensar en un operator como un objeto que encapsula la tarea (task), el job que quieres ejecutar. Por ejemplo, quieres conectarte a tu base de datos e insertar datos en ella. Puedes utilizar un operator especial para hacerlo. Así que, básicamente, un operator es una task en tu DAG, en tu data pipeline.\n",
    "\n",
    "**`Hay tres tipos diferentes de operators`** y el primero son los action operators.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/MKXVZnPd/a7.png\"></center>\n",
    "\n",
    "- Los **`action operators`** son operadores encargados de ejecutar algo. Por ejemplo, quieres ejecutar una función de python, usarás el operador Python (PythonOperator), quieres ejecutar un comando bash, ejecutarás el operador bash (BashOperator). Si quieres ejecutar una sequence request, usarás el operador postgresql. Ese es el action operator.\n",
    "\n",
    "- Luego, tienes el **`transfer operator`** que también te permite transferir datos de un origen a un destino como el operador Presto a MySql. Y hay una tonelada de operadores de transferencia disponibles en Airflow, pero tenga en cuenta que un transfer operator es un operador que transfiere datos de una fuente a un destino.\n",
    "\n",
    "- Por último, pero no menos importante, y que es mi categoría favorita de operadores, los sensores. Un **`sensor operator`** te permite esperar a que algo suceda antes de avanzar, antes de completarse. Por ejemplo, quieres esperar a que un archivo llegue a una ubicación específica en tu file system, puedes usar el file sensor. Si quiere esperar a que se produzca un registro específico en su base de datos antes de pasar a la siguiente tarea, puede utilizar el sensor sql que le permite esperar a que se cumpla una condición antes de completarse.\n",
    "\n",
    "Ahora, ya sabes qué es un DAG y qué es un operador. Tienes que conocer dos conceptos más.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`Task | Task Instance `**\n",
    "\n",
    "El primero es una task instance. Bien, una instancia de tarea no es más que una instancia de tu operador en tu DAG. **`Así que, básicamente, cuando un operador se ejecuta en tu DAG, se convierte en una task instance`**.\n",
    "\n",
    "De nuevo, un Operator es una task y cuando se ejecuta un operator, o sea una task, obtienes una task instance. Es tan simple como eso.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/tCfPs98y/a8.png\"></center>\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/W1kZrQPZ/a9.png\"></center>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`Workflow`**\n",
    "\n",
    "Y, por último, la combinación de todos los conceptos que usted ha visto hasta ahora le da el concepto de workflow, tus operators a las dependencias entre sus operators por lo que sus tasks dan el DAG y esto es un workflow.\n",
    "\n",
    "Así que la combinación de todos los conceptos, DAG, operador, dependencia te dan el workflow.\n",
    "\n",
    "Por último, si se agrupan todos esos conceptos, se llega al concepto de workflow donde tienes tus Operators, directed dependencies, es decir, un DAG, y eso te da el concepto de Workflow. Es la combinación de todos los conceptos.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/8CdrXMcP/a10.png\"></center>\n",
    "\n",
    "OK, ahora estamos en lo último. Y creo que la pregunta más importante que tienes que responder es lo que no es Airflow. Bueno, Airflow no es una solución de data streaming. Tampoco es un framework de procesamiento de datos, lo que significa que no debes procesar terabytes o gigabytes de datos en tu DAG en Airflow.\n",
    "\n",
    "En su lugar, si tienes terabytes o gigabytes de datos para procesar, utilizarás el operador SparkSubmit, que desencadenará (trigger) un Spark job y procesarás tus terabytes de datos en ese Spark job, pero no en airflow.\n",
    "\n",
    "Recuerda esto, piensa en airflow como una forma de disparar (trigger) herramientas externas y el procesamiento de tus datos, lo harás en tus herramientas externas como spark, flink o lo que quieras.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/sXVGkxGP/a11.png\"></center>\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
