{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4 - Resumen**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`Preguntas resumen`**\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/tgp6y963/a39.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/Dz6XwRQt/a40.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/3RwD49nW/a41.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/tJZZDJ0j/a42.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/cCpKzWZ8/a43.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/xCFNKH45/a44.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`Puntos clave`**\n",
    "\n",
    "**`¿Qué debes tener en cuenta después de lo que ha aprendido?`**\n",
    "\n",
    "-\tAirflow es un orquestador, no un framework de procesamiento. Procese sus gigabytes de datos fuera de Airflow (es decir, tiene un Spark cluster, utiliza un Operator para ejecutar un Spark job, y los datos se procesan en Spark).\n",
    "\n",
    "-\tUn DAG es un data pipeline, un Operator es una tarea.\n",
    "\n",
    "-\tUn Executor define cómo se ejecutan tus tareas, mientras que un Worker es un proceso que ejecuta tu tarea\n",
    "\n",
    "-\tEl Scheduler programa sus tareas, el Web Server sirve la UI, y la base de datos almacena los metadatos de Airflow.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
