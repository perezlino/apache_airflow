{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.6 - The Current Config**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el archivo docker compose oficial de airflow, terminas con el Celery executor, lo que significa que ejecutas tus tareas en un cluster de Celery en diferentes workers.\n",
    "\n",
    "Déjame mostrarte esto.\n",
    "\n",
    "Si abres el archivo docker-composer.yaml y te desplazas un poco, puedes ver el executor utilizado, que es el Celery executor. Luego tienes la base de datos Postgres y el Result Backend, que también es Postgres.\n",
    "\n",
    "Recuerda que el Result Backend es donde se almacenan los estados de las tareas por parte de Celery y el Broker URL, que es la cola donde se ponen las tareas por parte del Scheduler para ejecutarlas en los workers.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/wT2NCSJ3/a339.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/pLdjRqYC/a340.png\"></center>\n",
    "<center><img src=\"https://i.postimg.cc/7ZQ260Kk/a341.png\"></center>\n",
    "\n",
    "y el worker Airflow. Para ejecutar un worker de Airflow y ejecutar sus tareas en él, basta con ejecutar el comando airflow: celery worker. Y al ejecutar ese comando en una máquina determinada, esa máquina se convierte en un worker de Airflow en el que puedes ejecutar tareas. Es tan simple como eso. Así que si quieres tener más workers, sólo tienes que ejecutar ese comando en diferentes máquinas.\n",
    "\n",
    "Vamos a hacer eso en una sola máquina sólo para mostrarte cómo funciona. Pero eso es lo que se obtiene con el archivo docker compose de airflow.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/6Q2GGqPG/a342.png\"></center>\n",
    "\n",
    "Si ejecutas el comando docker compose ps en tu terminal, puedes ver un Scheduler, el trigger, el Web Server, un worker, flower, que es el dashboard para monitorear tus workers de airflow, la base de datos y Redis.\n",
    "\n",
    "Es bastante genial, ¿verdad?\n",
    "\n",
    "Me refiero a que con sólo ejecutar el comando docker compose starts, terminas con airflow corriendo en el executor Celery y así puedes ejecutar múltiples tareas al mismo tiempo.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/FR0dDqNr/a343.png\"></center>\n",
    "\n",
    "Y por cierto, para mostrarte cómo funciona, vamos a utilizar el parallel_dag y este dag es muy sencillo. Son sólo un par de tareas que no hacen nada más que esperar.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/5y1HG32w/a344.png\"></center>\n",
    "\n",
    "Y si usted mira a su instancia de airflow, usted terminará con este dag. Así que extract_a y extract_b deberían poder ejecutarse al mismo tiempo. Lo mismo load_a y load_b y finalmente transform. Bien, todo está listo. Vamos a pasar al siguiente video.\n",
    "\n",
    "<center><img src=\"https://i.postimg.cc/3NByHvkz/a345.png\"></center>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
